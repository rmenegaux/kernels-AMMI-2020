{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Let $A \\in \\mathbb{R}^{m\\times m}$ be an $m\\times m$ matrix, and $f$ defined as follows:\n",
    "$$\n",
    "f: \\begin{cases} \n",
    "      \\mathbb{R}^m \\to \\, ?\\\\\n",
    "      x \\to x^\\top A x  \\\\\n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "1. What is the dimension of the output space (codomain of $f$)?\n",
    "2. What is the Jacobian  $\\nabla f$?\n",
    "3. What is the Hessian  $\\nabla^2 f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "1. $x^\\top A x$ is a real number: **codomain is $\\color{green}{\\mathbb{R}}$**. ($\\mathbb{R}^{1\\times m} \\cdot \\mathbb{R}^{m\\times m} \\cdot \\mathbb{R}^{m\\times 1}$)\n",
    "2. - To find $\\nabla f$, we choose $i$ and regroup all terms that depend on $x_i$:\n",
    "$$f(x) = \\sum_{k,l} A_{kl} x_k x_l = \\sum_{k}(A_{ik}+A_{ki})x_kx_i+ \\text{ terms that do not depend on }x_i$$\n",
    "So $$\\frac{\\partial f}{\\partial x_i} = \\sum_{k}(A_{ik}+A_{ki})x_k = ((A+A^\\top)x)_i$$\n",
    "Hence $\\color{green}{\\nabla f(x) = (A+A^\\top)x}$\n",
    "   - Other method: Identifying the linear part in Taylor expansion.\n",
    "   Let $h \\in \\mathbb{R}^m$:\n",
    "   $$f(x+h) = f(x) + h^\\top \\nabla f(x) + o(\\|h\\|)$$\n",
    "   \n",
    "   In our case \n",
    "   $$\\begin{aligned}\n",
    "   f(x+h) &= (x+h)^\\top A (x+h) \\\\\n",
    "          &= f(x) + h^\\top Ax + x^\\top Ah + h^\\top Ah\\\\\n",
    "          &= f(x) + h^\\top \\color{green}{(A + A^\\top) x} + o(\\|h\\|)\n",
    "          \\end{aligned}$$\n",
    "          \n",
    "3. Seen in class (PS 1): $\\nabla^2f(x) = \\nabla \\nabla f(x) = A + A^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 2\n",
    "In what cases is hard margin SVM not applicable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "Hard margin SVM finds hyperplane that separate the data perfectly: when there is none there is no solution.\n",
    "**When the training data is not linearly separable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 3\n",
    "What is the Ridge Regression estimator $\\beta^{\\mathrm{ridge}}_\\lambda$ when:\n",
    "1. $\\lambda = 0$?\n",
    "2. $\\lambda = + \\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "Seen in class:\n",
    "1. No regularization: $\\hat{\\beta}^{ridge}_\\lambda = \\hat{\\beta}^{OLS}$\n",
    "2. Infinite regularization: $\\hat{\\beta}^{ridge}_\\lambda = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 4\n",
    "In general, what happens to the margin of SVM if C decreases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "Slide 86, the objective of SVM is:\n",
    "$$\\min_f \\left\\{\\frac{1}{margin(f)} + C\\times errors(f)\\right\\}$$\n",
    "\n",
    "Lesser $C$ means greater tolerance to errors so generally **greater margin**.\n",
    "\n",
    "Cf practical session 2\n",
    "\n",
    "*Rmk: Case where it does not change: If there are no errors for the new and old values of $C$  ($errors(f)=0$)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 5\n",
    "Explain (in words) what empirical risk is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "The empirical risk is the **average loss on the training dataset**. In general, classification: $0/1$ loss, regression: MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 5\n",
    "Why do we use a loss function (hinge, logistic etcâ€¦) in classification problems instead of optimising the empirical risk directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "- Empirical risk with $0/1$ loss is **hard to optimize** (gradient=0, typically NP-hard to solve)\n",
    "- Regularization term has no effect.\n",
    "- Loss functions $\\phi$ are chosen to have nice properties to optimize: convex, differentiable.\n",
    "- They are also chosen to be decreasing functions of the margin, which ensures\n",
    "$$\\text{small empirical risk} \\implies \\text{small empirical-}\\phi risk$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 6\n",
    "In machine learning, what method(s) are used to tune hyperparameters of models?\n",
    "\n",
    "(ex: $C$ in SVM, $\\lambda$ in logistic or ridge regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 7\n",
    "What happens to the stability of an estimator as we increase regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More regularization means **lesser variance** i.e. a more stable estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 8\n",
    "Let $q\\in \\mathbb{R}^m$ and $c \\in \\mathbb{R}$.\n",
    "\n",
    "$A \\in \\mathbb{R}^{m \\times m}$ is a symmetric matrix, with strictly positive eigenvalues. We note $B = A^{-1}$.\n",
    "\n",
    "What is the solution $x^*$ to the following minimisation problem?\n",
    "$$\\min_{x\\in \\mathbb{R}^m} \\frac{1}{2} x^\\top A x + q^T x + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a convex quadratic problem, it has a unique solution for $\\nabla f = 0$\n",
    "\n",
    "$\\nabla f(x) = Ax + q$ (cf question 1. with $A$ symmetric)\n",
    "\n",
    "$\\nabla f(x^*) = 0 \\iff Ax^* = -q \\iff \\color{green}{x^* = -Bq}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 9\n",
    "In soft margin SVM, with parameter $C$, suppose you have found the solutions $\\alpha\\in \\mathbb{R}^n$ to the dual problem.\n",
    "1. How do you find the support vectors from $\\alpha$?\n",
    "2. You can obtain $w$ from $\\alpha$ like this: $w = \\sum_{i=1}^{n} \\alpha_i y_i x_i$. How do you obtain the intercept $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Support vectors are points $i^*$ such that $\\color{green}{0 < \\alpha_{i^*} < C}$ (Implementation we can use a small numerical tolerance $\\epsilon \\leq \\alpha_{i^*} \\leq C - \\epsilon$)\n",
    "\n",
    "2. For all support vectors $i^*$, $\\color{green}{b = y_{i^*} - w^\\top x_{i^*}}$. One can do the average over all the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 10\n",
    "The solution of ridge regression with no intercept can be obtained in two ways: \n",
    "\n",
    "  - $w = ( X^\\top X + \\lambda I)^{-1} X^\\top y\\,\\,\\,$   <font color=\"green\">(1)</font>\n",
    "  \n",
    "  - $w = X (X X^\\top + \\lambda I)^{-1} y\\,\\,\\,\\,\\,\\,$    <font color=\"green\">(2)</font>\n",
    "\n",
    "Which formula (<font color=\"green\">1</font> or <font color=\"green\">2</font>) would you use, and in which situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "$X$ is an $n\\times p$ matrix so $XX^\\top$ is $n\\times n$ and $X^\\top X$ is $p\\times p$.\n",
    "\n",
    "Inverting a matrix is an expensive operation ($O(N^3)$) so we choose:\n",
    "- **(1) if ** $\\color{green}{p<n}$: less features than data points\n",
    "- **(2) if ** $\\color{green}{n<p}$: more data points than features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 11\n",
    "\n",
    "For the soft-margin SVM with no intercept, the prediction function is $f(x) = w^\\top x$ instead of  $f(x) = w^\\top x + b$.\n",
    "\n",
    "1. How does this change the dual problem?\n",
    "2. The coordinate descent method consists of iteratively optimizing over one variable while keeping the other variables fixed. Optimize the dual problem of question 1. with respect to $\\alpha_j$ and give the update rule for the coordinate descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Solution\n",
    "1. slide 77: the **equality constraint $\\sum \\alpha_i y_i = 0$ disappears**\n",
    "\n",
    "2. Dual objective: $$L(\\alpha) = \\sum_i\\alpha_i - \\frac{1}{2}\\sum_{ij} \\alpha_i \\alpha_j y_i y_j x_i^\\top x_j$$\n",
    "Derive $L(\\alpha)$ w.r.t. $\\alpha_j$:\n",
    "$$\\frac{\\partial L}{\\partial \\alpha_j} = 1 - y_j x_j^\\top \\sum_{i\\neq j}\\alpha_i y_i  x_i - \\|x_j\\|^2 \\alpha_j$$\n",
    "The optimal $\\alpha_j^*$ is obtained by setting this derivative to 0:\n",
    "$$\\color{green}{\\alpha_j^* = \\frac{1 - y_j x_j^\\top \\sum_{i\\neq j}\\alpha_i y_i  x_i}{\\|x_j\\|^2}}$$\n",
    "\n",
    "The update rule for the coordinate descent is then simply:\n",
    "$$\\alpha_j^{new} \\leftarrow \\alpha_j^*$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 12\n",
    "\n",
    "Give the update rule to learn a soft-margin SVM classifier with the stochastic gradient descent method. The update rule should be for the primal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "The soft-margin SVM primal objective is (slide 87):\n",
    "$$\\min_{w,b} \\frac{1}{2}\\|w\\|^2 + C \\sum_i \\max (0, 1 - y_i(w^\\top x_i + b))$$\n",
    "\n",
    "Let us set $\\mathbf{l} := \\left[\\mathbb{1}_{\\{y_i(w^\\top x_i + b) < 1\\}}\\right] \\in \\mathbb{R}^n$, and rewrite the loss function:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L(w, b) &= \\frac{1}{2}\\|w\\|^2 + C \\, \\mathbf{l}^\\top \\left[ 1 - y_i(w^\\top x_i + b)\\right]_i\\\\\n",
    "        &= \\frac{1}{2}\\|w\\|^2 - C \\, \\mathbf{l}^\\top \\tilde{X}^\\top w - C(\\mathbf{l}^\\top y) \\, b \\, + \\, C \\mathbf{l}^\\top 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "where we have set $\\tilde{X} = \\left[y_1 x_1, ..., y_n x_n\\right]^\\top \\in \\mathbb{R}^{n\\times p}$\n",
    "\n",
    "We compute the gradients with respect to $w$ and $b$:\n",
    "$$\\color{green}{\\begin{aligned}\n",
    "&\\nabla_w L = w - C \\, \\tilde{X} \\, \\mathbf{l} = w - C \\sum y_i \\mathbb{1}_{\\{y_i(w^\\top x_i + b) < 1\\}} \\, x_i\\\\\n",
    "&\\nabla_b L = - C(\\mathbf{l}^\\top y) = - C \\sum y_i \\, \\mathbb{1}_{\\{y_i(w^\\top x_i + b) < 1\\}}\n",
    "\\end{aligned}}$$\n",
    "\n",
    "For a learning rate $\\eta$, the udpate rule is then:\n",
    "$$\\begin{aligned}\n",
    "w^{new} &\\leftarrow w^{old} - \\eta \\nabla_w L(w^{old}, b^{old}) \\\\\n",
    "b^{new} &\\leftarrow b^{old} - \\eta \\nabla_b L(w^{old}, b^{old})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 14\n",
    "\n",
    "Is it possible to perform multi-class classification using ridge regression? If yes, how would you proceed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have $N$ classes $\\{1, ..., N\\}$.\n",
    "\n",
    "We one-hot encode the response variable $y = (\\mathbb{1}_{class=i})_i \\in \\mathbb{R}^{N}$, then train a Ridge Regression, with parameter $\\beta \\in \\mathbb{R}^{p\\times N}$.\n",
    "\n",
    "The classifying rule will then be $\\hat{y} = \\mathrm{argmax} \\, \\beta^\\top x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 15\n",
    "\n",
    "***\n",
    "How could you perform multi-class classification with logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
